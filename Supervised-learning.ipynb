{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 635 - Introduction to Machine learning - Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook discusses the training of neural networks to be able to perform sentiment analysis over a input review of video game. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "\n",
    "dataset = pd.read_csv(\"Video_Games_CSV.csv\")\n",
    "\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text data preprocessing utility functions are mentioned below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A utility function to remove non alphabetical characters from the text.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def clean_noncharacters(text):\n",
    "    # Removing punctuations\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    # Removing numerics\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "A utility function to remove URL links from the text.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def clean_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "A utility function to remove stopwords from the text.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def clean_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    res = [w for w in text.split() if not w in stop_words]\n",
    "    res_string = \" \".join(str(x) for x in res)\n",
    "    return res_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply above preprocessing methods to the dataset\n",
    "\n",
    "dataset[\"reviewText\"] = dataset[\"reviewText\"].apply(lambda x : clean_noncharacters(str(x)))\n",
    "dataset[\"reviewText\"] = dataset[\"reviewText\"].apply(lambda x : clean_URL(str(x)))\n",
    "dataset[\"reviewText\"] = dataset[\"reviewText\"].apply(lambda x : clean_stopwords(str(x)))\n",
    "\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As studied previously, not all reviews have numeric overall rating class. Some have textual classes as well. So we eliminate those reviews from our training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the rows with \"overall\" values not in range 1.0-5.0\n",
    "\n",
    "dataset = dataset[dataset['overall'].apply(lambda x: x in [\"1.0\", \"2.0\", \"3.0\", \"4.0\", \"5.0\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing all the columns which do not play role in training of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortening the dataset by removing all the columns except reviewText and overall\n",
    "\n",
    "dataset_short = dataset[[\"reviewText\",\"overall\"]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assign a polarity value to each review which determines the overall sentiment of the review.\n",
    "\n",
    "The mapping is as below:\n",
    "\n",
    "- Overall rating > 3: Positive sentiment (+1)\n",
    "- Overall rating == 3: Neutral sentiment (0)\n",
    "- Overall rating < 3: Negative sentiment (-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_polarity(row):\n",
    "    if int(float(row[\"overall\"])) > 3:\n",
    "        return 1\n",
    "    elif int(float(row[\"overall\"])) == 3:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_short['polarity'] = dataset_short.apply(lambda row: apply_polarity(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_short.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A utlity function to plot the learning curves for a neural network history instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the data to be able to train by a neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the tokenizer and pad_sequences libraries\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "# Setting the random seed so that rows shuffle in the same way in every session.\n",
    "\n",
    "tf.random.set_seed(100)\n",
    "\n",
    "\n",
    "# vocab_size - size of the vocabulary (unique words in the data corpus)\n",
    "# trunc_type - whether to truncate the sentence from behind or start (in case if sentence length\n",
    "# padding_type - whether to pad the short sentences from behind or start\n",
    "# oov_tok - replace the out of vocab word with a token\n",
    "# training_size - size of training data\n",
    "# max_length - maximum length of a sentence sequence\n",
    "\n",
    "vocab_size = 100\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV_TOKEN>\"\n",
    "training_size = int(len(dataset_short)*0.6)\n",
    "max_length = 100\n",
    "\n",
    "\n",
    "# Shuffle the rows\n",
    "dataset_short = dataset_short.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Splitting the train and the test sentences list.\n",
    "\n",
    "temp = [str(x) for x in dataset_short[\"reviewText\"].tolist()]\n",
    "train_reviews = temp[:training_size]\n",
    "test_reviews = temp[training_size:]\n",
    "\n",
    "# Splitting the train and the test labels list.\n",
    "\n",
    "temp2 = [x for x in dataset_short[\"polarity\"].tolist()]\n",
    "train_rating = temp2[:training_size]\n",
    "test_rating = temp2[training_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the words\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_reviews)\n",
    "\n",
    "\n",
    "# Convert sentence reviews to sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_reviews)\n",
    "# Convert sequences to padded sequences\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "\n",
    "# Convert sentence reviews to sequences\n",
    "test_sequences = tokenizer.texts_to_sequences(test_reviews)\n",
    "# Convert sequences to padded sequences\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting all the data to numpy arrays\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "train_padded = np.array(train_padded)\n",
    "train_rating = np.array(train_rating)\n",
    "test_padded = np.array(test_padded)\n",
    "test_rating = np.array(test_rating)\n",
    "\n",
    "\n",
    "train_rating_encoded = to_categorical(train_rating, num_classes = 3)\n",
    "test_rating_encoded = to_categorical(test_rating, num_classes = 3)\n",
    "\n",
    "train_padded = np.array(train_padded)\n",
    "train_rating_encoded = np.array(train_rating_encoded)\n",
    "\n",
    "test_padded = np.array(test_padded)\n",
    "test_rating_encoded = np.array(test_rating_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis using dense network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "dnn_model = tf.keras.Sequential([\n",
    "        \n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),    \n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(10, activation = \"relu\"),\n",
    "    tf.keras.layers.Dense(5, activation = \"relu\"),\n",
    "    tf.keras.layers.Dense(3, activation = \"softmax\")\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "dnn_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Uncomment below line for Hyperparamter tuning change\n",
    "# num_epochs = 25\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "# Learning rate scheduler callback function\n",
    "def scheduler(num_epochs):\n",
    "  if(num_epochs < 3):\n",
    "    return 0.001\n",
    "  else:\n",
    "    return 0.0001 * tf.math.exp(0.1 * (10 - num_epochs))\n",
    "\n",
    "lr_scheduler_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "\n",
    "# Change the patience value to 2 for hyperparameter tuned model.\n",
    "\n",
    "early_stopping_callback_loss = tf.keras.callbacks.EarlyStopping(monitor = 'loss', patience = 1)\n",
    "\n",
    "# Change the patience value to 2 for hyperparameter tuned model.\n",
    "\n",
    "early_stopping_callback_val_loss = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 1)\n",
    "\n",
    "\n",
    "# Add lr_scheduler_callback to the callbacks list function parameter below to enable learning rate scheduler decay.\n",
    "\n",
    "dnn_history = dnn_model.fit(train_padded, train_rating_encoded, epochs=num_epochs, validation_split = 0.2, \\\n",
    "                         callbacks = [early_stopping_callback_loss, early_stopping_callback_val_loss \\\n",
    "                                      ], verbose=1)\n",
    "\n",
    "plot_graphs(dnn_history, \"accuracy\")\n",
    "plot_graphs(dnn_history, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_classes = []\n",
    "test_predictions = dnn_model.predict(test_padded)\n",
    "\n",
    "# Assigning the suitable class to the test data from 3 softmax class probabilities.\n",
    "\n",
    "i = 0\n",
    "for probs in test_predictions:\n",
    "    if(probs[0] > probs[1] and probs[0] > probs[2]):\n",
    "        test_pred_classes.append(0)\n",
    "    if(probs[1] > probs[0] and probs[1] > probs[2]):\n",
    "        test_pred_classes.append(1)\n",
    "    if(probs[2] > probs[0] and probs[2] > probs[1]):\n",
    "        test_pred_classes.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"The accuracy of the model is: {}\".format(accuracy_score(test_rating, test_pred_classes)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using LSTM based network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "lstm_model = tf.keras.Sequential([\n",
    "    \n",
    "    \n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),    \n",
    "    tf.keras.layers.LSTM(18),\n",
    "    tf.keras.layers.Dense(10, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(3, activation = \"softmax\")\n",
    "    \n",
    "    ])\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Uncomment below line for Hyperparamter tuning change\n",
    "# num_epochs = 25\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "# Learning rate scheduler callback function\n",
    "def scheduler(num_epochs):\n",
    "  if(num_epochs < 3):\n",
    "    return 0.001\n",
    "  else:\n",
    "    return 0.0001 * tf.math.exp(0.1 * (10 - num_epochs))\n",
    "\n",
    "lr_scheduler_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "\n",
    "\n",
    "# Change the patience value to 2 for hyperparameter tuned model.\n",
    "\n",
    "early_stopping_callback_loss = tf.keras.callbacks.EarlyStopping(monitor = 'loss', patience = 1)\n",
    "\n",
    "# Change the patience value to 2 for hyperparameter tuned model.\n",
    "\n",
    "early_stopping_callback_val_loss = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 1)\n",
    "\n",
    "\n",
    "\n",
    "# Add lr_scheduler_callback to the callbacks list function parameter below to enable learning rate scheduler decay.\n",
    "\n",
    "lstm_history = lstm_model.fit(train_padded, train_rating_encoded, epochs=num_epochs, validation_split = 0.2, \\\n",
    "                         callbacks = [early_stopping_callback_loss, early_stopping_callback_val_loss, lr_scheduler_callback], verbose=1)\n",
    "\n",
    "plot_graphs(lstm_history, \"accuracy\")\n",
    "plot_graphs(lstm_history, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_classes = []\n",
    "test_predictions = lstm_model.predict(test_padded)\n",
    "\n",
    "# Assigning the suitable class to the test data from 3 softmax class probabilities.\n",
    "\n",
    "i = 0\n",
    "for probs in test_predictions:\n",
    "    if(probs[0] > probs[1] and probs[0] > probs[2]):\n",
    "        test_pred_classes.append(0)\n",
    "    if(probs[1] > probs[0] and probs[1] > probs[2]):\n",
    "        test_pred_classes.append(1)\n",
    "    if(probs[2] > probs[0] and probs[2] > probs[1]):\n",
    "        test_pred_classes.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"The accuracy of the model is: {}\".format(accuracy_score(test_rating, test_pred_classes)*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
